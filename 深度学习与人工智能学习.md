#  深度学习与人工智能学习

## 1 引言

1. 机器学习的一般流程：

![image-20221005171107893](https://gitee.com/QishengStudent/figs/raw/master/image-20221005171107893.png)

2. 核心部分：

![image-20221006100737059](https://gitee.com/QishengStudent/figs/raw/master/image-20221006100737059.png)

3. 机器学习套路：

![image-20221006101037860](https://gitee.com/QishengStudent/figs/raw/master/image-20221006101037860.png)

3. 一个数据集：CIFAR-10。

## 2 神经网络基础

 神经网络：通过神经网络这个黑盒子，实现从输入到输出的映射。例如对于一个图像分类任务而言，看下面这个例子，输入是一张猫的图片，输出则是属于每个类别的得分。神经网络这个黑盒子实质上是一个映射函数$f(x, W)$，$x$是输入的图片，图片实质上是一系列像素点（特征），不同的像素点对于不同类别的动物而言其重要程度不同（有些促进、有些抑制）$，W$就表示了图片中每个像素点对于类别判断的重要程度，即权重。特征维度与权重维度是一致的。其中，对于神经网络而言，输入的特征是不变的，我们需要调整的是权重参数，使得它符合该任务。权重$W$的初始值可以是随机初始化的。

![image-20221006130933053](https://gitee.com/QishengStudent/figs/raw/master/image-20221006130933053.png)

再次强调，$W$的维度和$x$是一致的，但是$w_i$的个数与分类任务的类别数一致。在神经网络中，还有一个偏置量$b$，用于微调每个类别的得分。

![image-20221006132142634](https://gitee.com/QishengStudent/figs/raw/master/image-20221006132142634.png)

神经网络的任务：调整$W$，使得其适应任务。

![image-20221006132451441](https://gitee.com/QishengStudent/figs/raw/master/image-20221006132451441.png)

决策边界

![image-20221006132534938](https://gitee.com/QishengStudent/figs/raw/master/image-20221006132534938.png)

总结：通过输入的$x$和权重矩阵$W$计算出各个类别的得分，这个过程称为前向传播。

但是由于一开始的权重矩阵$W$是随机初始化的，得到的分类结果不一定正确。往往使用**损失函数**来衡量分类的结果。

PS：神经网络既能做分类，也能做回归。二者的网络结构是不变的，区别在于其损失函数的不同。回归任务由得分值计算损失，分类任务由概率值计算损失。

损失函数的例子：

![image-20221006151405841](https://gitee.com/QishengStudent/figs/raw/master/image-20221006151405841.png)

图中的损失函数的含义为：利用$ \sum _{j\neq {y}_{i}} max{(0,\, 错误类别的得分 - 正确类别的得分 + \Delta)} $。每一项的含义为正确类别的得分要大于错误类别的得分加$\Delta$，才可以说没有损失（加上常数是为了进一步增加模型在区 分相似目标上的能力）。所加的常数$\Delta$表示这个模型对错分类的容忍度。

损失函数的改进：

![image-20221006153159788](https://gitee.com/QishengStudent/figs/raw/master/image-20221006153159788.png)

损失函数的值相同，并不意味着两个模型一样。

改进：加上正则化惩罚项。其中$\lambda$惩罚项系数。

![image-20221006153442953](https://gitee.com/QishengStudent/figs/raw/master/image-20221006153442953.png)

 总结：

![image-20221006153545664](https://gitee.com/QishengStudent/figs/raw/master/image-20221006153545664.png)

把得分值映射到概率空间：

![image-20221006153633111](https://gitee.com/QishengStudent/figs/raw/master/image-20221006153633111.png)

![image-20221006153649187](https://gitee.com/QishengStudent/figs/raw/master/image-20221006153649187.png)

使用对数$e$进一步放大得分值之间的差异。

从输入数据到得到损失这个过程称为前向传播。

### 反向传播

![image-20221006154830120](https://gitee.com/QishengStudent/figs/raw/master/image-20221006154830120.png)

![image-20221006154841750](https://gitee.com/QishengStudent/figs/raw/master/image-20221006154841750.png)

 

![image-20221006155408057](https://gitee.com/QishengStudent/figs/raw/master/image-20221006155408057.png)

反向传播要逐层计算。即

![image-20221006155440936](https://gitee.com/QishengStudent/figs/raw/master/image-20221006155440936.png)

![image-20221006155545502](https://gitee.com/QishengStudent/figs/raw/master/image-20221006155545502.png)

![image-20221006155945623](https://gitee.com/QishengStudent/figs/raw/master/image-20221006155945623.png)

![image-20221006160000482](https://gitee.com/QishengStudent/figs/raw/master/image-20221006160000482.png)

![image-20221006160209201](https://gitee.com/QishengStudent/figs/raw/master/image-20221006160209201.png)

![image-20221006160250182](https://gitee.com/QishengStudent/figs/raw/master/image-20221006160250182.png)

 `input layer`圆圈个数表述输入数据的特征维度数。

全连接：上一层的神经元和下一层的所有神经元都有连接。

隐层是把原始输入的特征数据进行一定的特征变换，使得到的特征更有利于计算机识别。

神经元之间的箭头表示的实质上是权重矩阵。

非线性：指的是在每次$xW_i$变化之后，都进行一次非线性变化，然后再进行权重计算。

神经元的个数对神经网络性能的影响：在一定程度内增加神经元的个数可以提升网络性能，但要注意过多的神经元可能导致过拟合的问题。

增加一个神经元对神经网络参数的影响：增加了一组参数，而不一定是一个参数。

![image-20221006161049681](https://gitee.com/QishengStudent/figs/raw/master/image-20221006161049681.png)

![image-20221006161435989](https://gitee.com/QishengStudent/figs/raw/master/image-20221006161435989.png)

![image-20221013194217174](https://gitee.com/QishengStudent/figs/raw/master/image-20221013194217174.png)

问题：学习率与正则化的区别？

![image-20221013194256647](https://gitee.com/QishengStudent/figs/raw/master/image-20221013194256647.png)

![image-20221013194318042](https://gitee.com/QishengStudent/figs/raw/master/image-20221013194318042.png)

激活函数：实现非线性变换。

![image-20221013194351353](https://gitee.com/QishengStudent/figs/raw/master/image-20221013194351353.png)

`Sigmoid`函数会存在梯度消失的现象。

![image-20221013194627967](https://gitee.com/QishengStudent/figs/raw/master/image-20221013194627967.png)

![image-20221013194712796](https://gitee.com/QishengStudent/figs/raw/master/image-20221013194712796.png)

![image-20221013194848034](https://gitee.com/QishengStudent/figs/raw/master/image-20221013194848034.png)

防止过拟合的方法：正则化、`drop_out`、控制神经元个数等。

 `drop_out`：在神经网络的训练过程中，对于神经元，按照一定的概率将其暂时从网络中丢弃。在训练完后，所有神经元均参与测试。

传统网络与CNN的辨析：传统网络输入会把特征拉成一个向量（一维的），CNN直接输入数据（$h*w*c$）。

![image-20221014110145360](https://gitee.com/QishengStudent/figs/raw/master/image-20221014110145360.png)

![image-20221014110300187](https://gitee.com/QishengStudent/figs/raw/master/image-20221014110300187.png)

![image-20221014110732266](https://gitee.com/QishengStudent/figs/raw/master/image-20221014110732266.png)

卷积做了一件什么事：将原始的图像进行一定的分块，然后对每一个小块学习到一个合适的权重矩阵，得到这个小块的代表特征（背景：一副图像不同部分对于图像的表意是不同的）。

![image-20221019212626020](https://gitee.com/QishengStudent/figs/raw/master/image-20221019212626020.png)

![image-20221019212533533](https://gitee.com/QishengStudent/figs/raw/master/image-20221019212533533.png)

![image-20221019213456082](https://gitee.com/QishengStudent/figs/raw/master/image-20221019213456082.png)

如上图所示，对于一张RGB三通道的彩色图像，选用了大小为$3×3$的卷积核分别对R、G、B这三个通道进行特征提取，在对每一通道的特征提取完毕之后将它们相加，并最后加上一个偏置量$b$，得到最终的特征值。

假设顺序的通道分别对应R、G、B，计算过程如下：

```bash
# 卷积核的运算是一个点乘（内积）过程，不是矩阵乘
# R通道计算过程
1 * （-1） + 1 * 0 + 1 * 1 + 1 * 0 = 0
# 
1 * 0 + 2 * 1 + 2 * 0 = 2
# 
2 * 0 + 2 * 0 = 0
# 合起来
0 + 2 + 0 = 2
# 加偏置量
2 + 1 = 3 
```



