# 深度学习与人工智能学习

## 1 引言

1. 机器学习的一般流程：

![image-20221005171107893](https://gitee.com/QishengStudent/figs/raw/master/image-20221005171107893.png)

2. 核心部分：

![image-20221006100737059](https://gitee.com/QishengStudent/figs/raw/master/image-20221006100737059.png)

3. 机器学习套路：

![image-20221006101037860](https://gitee.com/QishengStudent/figs/raw/master/image-20221006101037860.png)

3. 一个数据集：CIFAR-10。

## 2 神经网络基础

 神经网络：通过神经网络这个黑盒子，实现从输入到输出的映射。例如对于一个图像分类任务而言，看下面这个例子，输入是一张猫的图片，输出则是属于每个类别的得分。神经网络这个黑盒子实质上是一个映射函数$f(x, W)$，$x$是输入的图片，图片实质上是一系列像素点（特征），不同的像素点对于不同类别的动物而言其重要程度不同（有些促进、有些抑制）$，W$就表示了图片中每个像素点对于类别判断的重要程度，即权重。特征维度与权重维度是一致的。其中，对于神经网络而言，输入的特征是不变的，我们需要调整的是权重参数，使得它符合该任务。权重$W$的初始值可以是随机初始化的。

![image-20221006130933053](https://gitee.com/QishengStudent/figs/raw/master/image-20221006130933053.png)

再次强调，$W$的维度和$x$是一致的，但是$w_i$的个数与分类任务的类别数一致。在神经网络中，还有一个偏置量$b$，用于微调每个类别的得分。

![image-20221006132142634](https://gitee.com/QishengStudent/figs/raw/master/image-20221006132142634.png)

神经网络的任务：调整$W$，使得其适应任务。

![image-20221006132451441](https://gitee.com/QishengStudent/figs/raw/master/image-20221006132451441.png)

决策边界

![image-20221006132534938](https://gitee.com/QishengStudent/figs/raw/master/image-20221006132534938.png)

总结：通过输入的$x$和权重矩阵$W$计算出各个类别的得分，这个过程称为前向传播。

但是由于一开始的权重矩阵$W$是随机初始化的，得到的分类结果不一定正确。往往使用**损失函数**来衡量分类的结果。

PS：神经网络既能做分类，也能做回归。二者的网络结构是不变的，区别在于其损失函数的不同。回归任务由得分值计算损失，分类任务由概率值计算损失。

损失函数的例子：

![image-20221006151405841](https://gitee.com/QishengStudent/figs/raw/master/image-20221006151405841.png)

图中的损失函数的含义为：利用$ \sum _{j\neq {y}_{i}} max{(0,\, 错误类别的得分 - 正确类别的得分 + \Delta)} $。每一项的含义为正确类别的得分要大于错误类别的得分加$\Delta$，才可以说没有损失（加上常数是为了进一步增加模型在区 分相似目标上的能力）。所加的常数$\Delta$表示这个模型对错分类的容忍度。

损失函数的改进：

![image-20221006153159788](https://gitee.com/QishengStudent/figs/raw/master/image-20221006153159788.png)

损失函数的值相同，并不意味着两个模型一样。

改进：加上正则化惩罚项。其中$\lambda$惩罚项系数。

![image-20221006153442953](https://gitee.com/QishengStudent/figs/raw/master/image-20221006153442953.png)

 总结：

![image-20221006153545664](https://gitee.com/QishengStudent/figs/raw/master/image-20221006153545664.png)

把得分值映射到概率空间：

![image-20221006153633111](https://gitee.com/QishengStudent/figs/raw/master/image-20221006153633111.png)

![image-20221006153649187](https://gitee.com/QishengStudent/figs/raw/master/image-20221006153649187.png)

使用对数$e$进一步放大得分值之间的差异。

从输入数据到得到损失这个过程称为前向传播。

### 反向传播

![image-20221006154830120](https://gitee.com/QishengStudent/figs/raw/master/image-20221006154830120.png)

![image-20221006154841750](https://gitee.com/QishengStudent/figs/raw/master/image-20221006154841750.png)

 

![image-20221006155408057](https://gitee.com/QishengStudent/figs/raw/master/image-20221006155408057.png)

反向传播要逐层计算。即

![image-20221006155440936](https://gitee.com/QishengStudent/figs/raw/master/image-20221006155440936.png)

![image-20221006155545502](https://gitee.com/QishengStudent/figs/raw/master/image-20221006155545502.png)

![image-20221006155945623](https://gitee.com/QishengStudent/figs/raw/master/image-20221006155945623.png)

![image-20221006160000482](https://gitee.com/QishengStudent/figs/raw/master/image-20221006160000482.png)

![image-20221006160209201](https://gitee.com/QishengStudent/figs/raw/master/image-20221006160209201.png)

![image-20221006160250182](https://gitee.com/QishengStudent/figs/raw/master/image-20221006160250182.png)

 `input layer`圆圈个数表述输入数据的特征维度数。

全连接：上一层的神经元和下一层的所有神经元都有连接。

隐层是把原始输入的特征数据进行一定的特征变换，使得到的特征更有利于计算机识别。

神经元之间的箭头表示的实质上是权重矩阵。

非线性：指的是在每次$xW_i$变化之后，都进行一次非线性变化，然后再进行权重计算。

![image-20221006161049681](https://gitee.com/QishengStudent/figs/raw/master/image-20221006161049681.png)

![image-20221006161435989](https://gitee.com/QishengStudent/figs/raw/master/image-20221006161435989.png)